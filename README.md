Practical Homework 2 ‚Äî Support Vector Machines (SVM)

üìñ Overview

In this project, we explored Support Vector Machines (SVMs) to predict the likelihood of diabetes among adults using the 2022 NHIS dataset.
We handled real-world messy data, made modeling decisions based on critical thinking, and designed our workflow to mimic a real data science project ‚Äî focusing not just on performance, but clarity and correctness.

‚∏ª

 Methodology

1. Data Preparation
	‚Ä¢	Loaded the NHIS 2022 dataset.
	‚Ä¢	Replaced special codes (996, 997, 998, 999) with NA.
	‚Ä¢	Focused only on adults (Age ‚â• 18) with valid diabetes history (Yes/No responses).
	‚Ä¢	Selected an initial set of health and lifestyle variables (e.g., Age, BMI, Sleep, Fruit intake, Soda consumption).

2. Initial Modeling
	‚Ä¢	Trained a basic Linear SVM to check feasibility.
	‚Ä¢	Observed significant class imbalance (‚ÄúYes‚Äù class was much smaller), impacting model performance.
	‚Ä¢	Realized that some variables had low impact on prediction.

3. Feature Importance
	‚Ä¢	Extracted feature importance from the basic Linear SVM.
	‚Ä¢	Selected top 6 most influential features (Age, BMI, Alcohol Days, Moderate Exercise, Pizza Intake, Soda Intake) to focus modeling efforts.

4. Improved Modeling
	‚Ä¢	Added class weights to give more importance to minority class (‚ÄúYes‚Äù for diabetes).
	‚Ä¢	Retrained:
	‚Ä¢	Linear SVM (weighted + reduced features)
	‚Ä¢	Radial SVM (weighted)
	‚Ä¢	Polynomial SVM (weighted)

5. Tuning and Evaluation
	‚Ä¢	Performed hyperparameter tuning on each model (cost, gamma, degree).
	‚Ä¢	Evaluated using Accuracy, Precision, Recall, and F1 Score.
	‚Ä¢	Created mini datasets (AGE vs BMICALC) to plot decision boundaries for clear visualization.

‚∏ª
 Key Learnings
	‚Ä¢	Simply achieving high accuracy without considering class imbalance can be misleading.
	‚Ä¢	Feature selection based on importance simplified the models without major loss in performance.
	‚Ä¢	Radial SVM handled non-linear boundaries better than Linear SVM.
	‚Ä¢	Polynomial SVM struggled due to sensitivity to outliers and low generalization.
	‚Ä¢	Visualization of decision boundaries helped intuitively understand model behavior.

 Thought Process and Decisions
	‚Ä¢	Instead of blindly using all available features, we analyzed importance to reduce noise and complexity.
	‚Ä¢	Introduced class weights only after initial experiments showed clear imbalance issues.
	‚Ä¢	Focused on clean, interpretable plots (decision boundaries) using mini-datasets.
	‚Ä¢	Avoided overfitting by not making hyperparameter grids too complex.
	‚Ä¢	Took multiple iterations to fix errors ‚Äî ensuring logical progression rather than shortcutting.

 References
	1.	Cortes, C., & Vapnik, V. (1995). Support-vector networks. Machine Learning.
	2.	American Diabetes Association. (2022). Standards of Medical Care in Diabetes‚Äî2022.
	3.	James, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning (2nd ed.). Springer.
	4.	ChatGPT (2025). Helped troubleshoot errors, debug data flow, and generate initial background illustrations. OpenAI ChatGPT.
	5.	U.S. National Health Interview Survey (NHIS) 2022. Dataset Access.
